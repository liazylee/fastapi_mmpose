# app/pose_service/onnx_inference.py

import logging
import os
from typing import List, Tuple

import cv2
import numpy as np
import onnxruntime as ort

from app.pose_service.draw_pose_numba import _draw_circle_numba, _draw_line_numba, point_color, palette, skeleton, \
    link_color, default_skeleton, keypoint_colors
from app.video_service.permance_monitor import time_block

logger = logging.getLogger(__name__)


class ONNXPoseEstimator:
    def __init__(self, onnx_file: str, device: str = 'cuda') -> None:
        """åˆå§‹åŒ–ONNXå§¿æ€ä¼°è®¡å™¨

        Args:
            onnx_file: ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„
            device: æ¨ç†è®¾å¤‡ï¼Œ'cpu'æˆ–'cuda'
        """
        self.onnx_file = onnx_file
        self.device = device
        self.sess = self._build_session()
        # è·å–è¾“å…¥å°ºå¯¸
        h, w = self.sess.get_inputs()[0].shape[2:]
        self.model_input_size = (w, h)

    def _build_session(self) -> ort.InferenceSession:
        """æ„å»ºONNXè¿è¡Œæ—¶ä¼šè¯"""
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
        sess_options.intra_op_num_threads = os.cpu_count()  # å¯ä»¥æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´

        providers = ['CPUExecutionProvider'] if self.device == 'cpu' else ['CUDAExecutionProvider']
        sess = ort.InferenceSession(
            path_or_bytes=self.onnx_file,
            providers=providers,
            sess_options=sess_options  # æ·»åŠ è¿™ä¸ªå‚æ•°
        )
        return sess

    def inference(self, img: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """å¯¹å•å¼ å›¾åƒè¿›è¡Œå§¿æ€ä¼°è®¡

        Args:
            img: è¾“å…¥å›¾åƒï¼ŒBGRæ ¼å¼

        Returns:
            tuple: (å¯è§†åŒ–å›¾åƒï¼Œå…³é”®ç‚¹ï¼Œç½®ä¿¡åº¦åˆ†æ•°)
        """
        # 1. é¢„å¤„ç†
        resized_img, center, scale = self.preprocess(img)

        # 2. æ¨ç†
        outputs = self.run_inference(resized_img)

        # 3. åå¤„ç†
        keypoints, scores = self.postprocess(outputs, self.model_input_size, center, scale)

        # 4. å¯è§†åŒ–
        vis_img = self.visualize(img, keypoints, scores)

        return vis_img, keypoints, scores

    async def preprocess(self, img: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """é¢„å¤„ç†å›¾åƒ"""
        # è·å–å›¾åƒå½¢çŠ¶
        img_shape = img.shape[:2]
        bbox = np.array([0, 0, img_shape[1], img_shape[0]])

        # è·å–ä¸­å¿ƒå’Œç¼©æ”¾
        center, scale = self.bbox_xyxy2cs(bbox, padding=1.25)

        # è¿›è¡Œä»¿å°„å˜æ¢
        resized_img, scale = self.top_down_affine(self.model_input_size, scale, center, img)

        # å½’ä¸€åŒ–å›¾åƒ
        mean = np.array([123.675, 116.28, 103.53], dtype=np.float32)
        std = np.array([58.395, 57.12, 57.375], dtype=np.float32)
        resized_img = resized_img.astype(np.float32)  # ç¡®ä¿ä¸ºfloat32
        resized_img = (resized_img - mean) / std

        return resized_img, center, scale

    def run_inference(self, img: np.ndarray) -> List[np.ndarray]:
        """è¿è¡ŒONNXæ¨ç†"""
        # æ„å»ºè¾“å…¥
        input_tensor = [img.transpose(2, 0, 1)]

        # æ„å»ºè¾“å‡º
        sess_input = {self.sess.get_inputs()[0].name: input_tensor}
        sess_output = []
        for out in self.sess.get_outputs():
            sess_output.append(out.name)

        # è¿è¡Œæ¨¡å‹
        outputs = self.sess.run(sess_output, sess_input)

        return outputs

    async def run_inference_batch(self, batch_imgs: np.ndarray) -> List[np.ndarray]:
        """æ‰¹é‡è¿è¡ŒONNXæ¨ç†

        Args:
            batch_imgs: æ‰¹é‡å›¾åƒï¼Œå½¢çŠ¶ä¸º [N, H, W, C]

        Returns:
            æ‰¹é‡è¾“å‡ºåˆ—è¡¨
        """
        # ç¡®ä¿è¾“å…¥æ•°æ®ç±»å‹ä¸ºfloat32
        batch_imgs = batch_imgs.astype(np.float32)

        # è½¬æ¢ä¸º [N, C, H, W] æ ¼å¼
        input_tensor = batch_imgs.transpose(0, 3, 1, 2)

        # æ„å»ºè¾“å…¥
        sess_input = {self.sess.get_inputs()[0].name: input_tensor}
        sess_output = []
        for out in self.sess.get_outputs():
            sess_output.append(out.name)

        # è¿è¡Œæ¨¡å‹
        with time_block(name=f'onnx run batch', log_result=True):  # 10083
            outputs = self.sess.run(sess_output, sess_input)

        return outputs

    async def postprocess(self, outputs: List[np.ndarray], model_input_size: Tuple[int, int],
                          center: np.ndarray, scale: np.ndarray, simcc_split_ratio: float = 2.0
                          ) -> Tuple[np.ndarray, np.ndarray]:
        """åå¤„ç†ONNXæ¨¡å‹è¾“å‡º"""
        # ä½¿ç”¨simccè§£ç 
        simcc_x, simcc_y = outputs
        keypoints, scores = self.decode(simcc_x, simcc_y, simcc_split_ratio)

        # é‡æ–°ç¼©æ”¾å…³é”®ç‚¹
        keypoints = keypoints / model_input_size * scale + center - scale / 2

        return keypoints, scores

    def visualize_numba(self, img: np.ndarray, keypoints: np.ndarray, scores: np.ndarray,
                        thr: float = 0.4) -> np.ndarray:
        """NumbaåŠ é€Ÿçš„å§¿æ€å¯è§†åŒ–"""
        vis_img = img.copy()

        # ç›´æ¥å¤„ç†å…³é”®ç‚¹æ•°ç»„å’Œåˆ†æ•°æ•°ç»„
        for kpts, score in zip(keypoints, scores):
            keypoints_num = len(score)

            # ç»˜åˆ¶å…³é”®ç‚¹
            for i, (kpt, sc) in enumerate(zip(kpts, score)):
                if sc > thr:
                    x, y = int(kpt[0]), int(kpt[1])
                    if 0 <= x < vis_img.shape[1] and 0 <= y < vis_img.shape[0]:
                        color = keypoint_colors[i % len(keypoint_colors)]
                        _draw_circle_numba(vis_img, x, y, 3, color, -1)

            # ç»˜åˆ¶éª¨æ¶è¿æ¥
            for connection in default_skeleton:
                pt1_idx, pt2_idx = connection
                if (pt1_idx < keypoints_num and pt2_idx < keypoints_num and
                        score[pt1_idx] > thr and score[pt2_idx] > thr):

                    pt1 = (int(kpts[pt1_idx][0]), int(kpts[pt1_idx][1]))
                    pt2 = (int(kpts[pt2_idx][0]), int(kpts[pt2_idx][1]))

                    if (0 <= pt1[0] < vis_img.shape[1] and 0 <= pt1[1] < vis_img.shape[0] and
                            0 <= pt2[0] < vis_img.shape[1] and 0 <= pt2[1] < vis_img.shape[0]):
                        # ç¡®å®šè¿æ¥çº¿é¢œè‰²
                        color_idx = default_skeleton.index(connection) % len(keypoint_colors)
                        _draw_line_numba(vis_img, pt1[0], pt1[1], pt2[0], pt2[1], keypoint_colors[color_idx], 2)

        return vis_img

    def visualize(self, img: np.ndarray, keypoints: np.ndarray, scores: np.ndarray, thr: float = 0.4,
                  use_numba: bool = False) -> np.ndarray:

        """å¯è§†åŒ–å…³é”®ç‚¹å’Œéª¨æ¶"""
        vis_img = img.copy()
        if use_numba:
            vis_img = self.visualize_numba(vis_img, keypoints, scores, thr)
        # default color

        # ç»˜åˆ¶å…³é”®ç‚¹å’Œéª¨æ¶
        for kpts, score in zip(keypoints, scores):
            keypoints_num = len(score)
            for kpt, color in zip(kpts, point_color):
                cv2.circle(vis_img, tuple(kpt.astype(np.int32)), 1, palette[color], 1,
                           cv2.LINE_AA)
            for (u, v), color in zip(skeleton, link_color):
                if u < keypoints_num and v < keypoints_num \
                        and score[u] > thr and score[v] > thr:
                    cv2.line(vis_img, tuple(kpts[u].astype(np.int32)),
                             tuple(kpts[v].astype(np.int32)), palette[color], 1,
                             cv2.LINE_AA)

        return vis_img

    # ä»¥ä¸‹æ˜¯ä»åŸå§‹ä»£ç å¤åˆ¶çš„å·¥å…·å‡½æ•°
    def bbox_xyxy2cs(self, bbox: np.ndarray, padding: float = 1.) -> Tuple[np.ndarray, np.ndarray]:
        """å°†bboxæ ¼å¼ä»(x,y,w,h)è½¬æ¢ä¸º(center, scale)"""
        # è½¬æ¢å•ä¸ªbboxä»(4,)åˆ°(1,4)
        dim = bbox.ndim
        if dim == 1:
            bbox = bbox[None, :]

        # è·å–bboxä¸­å¿ƒå’Œç¼©æ”¾
        x1, y1, x2, y2 = np.hsplit(bbox, [1, 2, 3])
        center = np.hstack([x1 + x2, y1 + y2]) * 0.5
        scale = np.hstack([x2 - x1, y2 - y1]) * padding

        if dim == 1:
            center = center[0]
            scale = scale[0]

        return center, scale

    def _fix_aspect_ratio(self, bbox_scale: np.ndarray, aspect_ratio: float) -> np.ndarray:
        """æ‰©å±•æ¯”ä¾‹ä»¥åŒ¹é…ç»™å®šçš„çºµæ¨ªæ¯”"""
        w, h = np.hsplit(bbox_scale, [1])
        bbox_scale = np.where(w > h * aspect_ratio,
                              np.hstack([w, w / aspect_ratio]),
                              np.hstack([h * aspect_ratio, h]))
        return bbox_scale

    def _rotate_point(self, pt: np.ndarray, angle_rad: float) -> np.ndarray:
        """æ—‹è½¬ç‚¹"""
        sn, cs = np.sin(angle_rad), np.cos(angle_rad)
        rot_mat = np.array([[cs, -sn], [sn, cs]])
        return rot_mat @ pt

    def _get_3rd_point(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """è·å–ç¬¬ä¸‰ä¸ªç‚¹ï¼Œç”¨äºè®¡ç®—ä»¿å°„çŸ©é˜µ"""
        direction = a - b
        c = b + np.r_[-direction[1], direction[0]]
        return c

    def get_warp_matrix(self, center: np.ndarray, scale: np.ndarray, rot: float,
                        output_size: Tuple[int, int], shift: Tuple[float, float] = (0., 0.),
                        inv: bool = False) -> np.ndarray:
        """è®¡ç®—ä»¿å°„å˜æ¢çŸ©é˜µ"""
        shift = np.array(shift)
        src_w = scale[0]
        dst_w = output_size[0]
        dst_h = output_size[1]

        # è®¡ç®—å˜æ¢çŸ©é˜µ
        rot_rad = np.deg2rad(rot)
        src_dir = self._rotate_point(np.array([0., src_w * -0.5]), rot_rad)
        dst_dir = np.array([0., dst_w * -0.5])

        # è·å–æºçŸ©å½¢çš„å››ä¸ªè§’
        src = np.zeros((3, 2), dtype=np.float32)
        src[0, :] = center + scale * shift
        src[1, :] = center + src_dir + scale * shift
        src[2, :] = self._get_3rd_point(src[0, :], src[1, :])

        # è·å–ç›®æ ‡çŸ©å½¢çš„å››ä¸ªè§’
        dst = np.zeros((3, 2), dtype=np.float32)
        dst[0, :] = [dst_w * 0.5, dst_h * 0.5]
        dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir
        dst[2, :] = self._get_3rd_point(dst[0, :], dst[1, :])

        if inv:
            warp_mat = cv2.getAffineTransform(np.float32(dst), np.float32(src))
        else:
            warp_mat = cv2.getAffineTransform(np.float32(src), np.float32(dst))

        return warp_mat

    def top_down_affine(self, input_size: Tuple[int, int], bbox_scale: np.ndarray,
                        bbox_center: np.ndarray, img: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """é€šè¿‡ä»¿å°„å˜æ¢è·å–ä½œä¸ºæ¨¡å‹è¾“å…¥çš„bboxå›¾åƒ"""
        w, h = input_size
        warp_size = (int(w), int(h))

        # å°†bboxé‡å¡‘ä¸ºå›ºå®šçš„çºµæ¨ªæ¯”
        bbox_scale = self._fix_aspect_ratio(bbox_scale, aspect_ratio=w / h)

        # è·å–ä»¿å°„çŸ©é˜µ
        center = bbox_center
        scale = bbox_scale
        rot = 0
        warp_mat = self.get_warp_matrix(center, scale, rot, output_size=(w, h))

        # è¿›è¡Œä»¿å°„å˜æ¢
        img = cv2.warpAffine(img, warp_mat, warp_size, flags=cv2.INTER_LINEAR)

        return img, bbox_scale

    def get_simcc_maximum(self, simcc_x: np.ndarray, simcc_y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ä»simccè¡¨ç¤ºä¸­è·å–æœ€å¤§å“åº”ä½ç½®å’Œå€¼"""
        N, K, Wx = simcc_x.shape
        simcc_x = simcc_x.reshape(N * K, -1)
        simcc_y = simcc_y.reshape(N * K, -1)

        # è·å–æœ€å¤§å€¼ä½ç½®
        x_locs = np.argmax(simcc_x, axis=1)
        y_locs = np.argmax(simcc_y, axis=1)
        locs = np.stack((x_locs, y_locs), axis=-1).astype(np.float32)
        max_val_x = np.amax(simcc_x, axis=1)
        max_val_y = np.amax(simcc_y, axis=1)

        # è·å–xå’Œyè½´ä¹‹é—´çš„æœ€å¤§å€¼
        mask = max_val_x > max_val_y
        max_val_x[mask] = max_val_y[mask]
        vals = max_val_x
        locs[vals <= 0.] = -1

        # é‡å¡‘
        locs = locs.reshape(N, K, 2)
        vals = vals.reshape(N, K)

        return locs, vals

    def decode(self, simcc_x: np.ndarray, simcc_y: np.ndarray, simcc_split_ratio: float) -> Tuple[
        np.ndarray, np.ndarray]:
        """ç”¨é«˜æ–¯è°ƒåˆ¶simccåˆ†å¸ƒ"""
        keypoints, scores = self.get_simcc_maximum(simcc_x, simcc_y)
        keypoints /= simcc_split_ratio

        return keypoints, scores

    # æ·»åŠ åˆ° onnx_inference.py ä¸­ï¼Œç”¨äºæ€§èƒ½åˆ†æ

    def benchmark_detailed_performance(self):
        """è¯¦ç»†çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        import torch
        import time
        import GPUtil

        print("ğŸ” RTMPose ONNX æ€§èƒ½æ·±åº¦åˆ†æ")
        print("=" * 60)

        # 1. ç¯å¢ƒä¿¡æ¯
        print(f"ONNX Runtimeç‰ˆæœ¬: {ort.__version__}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹è¾“å…¥å°ºå¯¸: {self.model_input_size}")

        # GPUä¿¡æ¯
        if torch.cuda.is_available():
            gpu = GPUtil.getGPUs()[0]
            print(f"GPU: {gpu.name}")
            print(f"GPUå†…å­˜: {gpu.memoryTotal}MB")
            print(f"GPUé©±åŠ¨: {torch.version.cuda}")

        # 2. åˆ›å»ºæµ‹è¯•æ•°æ®
        dummy_img = np.random.randint(0, 255, (256, 192, 3), dtype=np.uint8).astype(np.float32)

        # å½’ä¸€åŒ–
        mean = np.array([123.675, 116.28, 103.53], dtype=np.float32)
        std = np.array([58.395, 57.12, 57.375], dtype=np.float32)
        dummy_img = (dummy_img - mean) / std

        # è½¬æ¢æ ¼å¼
        single_input = dummy_img.transpose(2, 0, 1)[np.newaxis, ...]  # [1, C, H, W]

        print(f"è¾“å…¥å½¢çŠ¶: {single_input.shape}")
        print(f"è¾“å…¥æ•°æ®ç±»å‹: {single_input.dtype}")

        # 3. å•å¸§æ€§èƒ½æµ‹è¯•
        print("\nğŸ“Š å•å¸§æ€§èƒ½æµ‹è¯•:")
        warmup_runs = 10
        test_runs = 50

        # é¢„çƒ­
        for _ in range(warmup_runs):
            _ = self.sess.run([out.name for out in self.sess.get_outputs()],
                              {self.sess.get_inputs()[0].name: single_input})

        # æµ‹è¯•å•å¸§
        start_time = time.time()
        for _ in range(test_runs):
            _ = self.sess.run([out.name for out in self.sess.get_outputs()],
                              {self.sess.get_inputs()[0].name: single_input})
        single_frame_time = (time.time() - start_time) / test_runs

        print(f"å•å¸§å¹³å‡æ—¶é—´: {single_frame_time * 1000:.2f}ms")
        print(f"å•å¸§FPS: {1 / single_frame_time:.1f}")

        # 4. ä¸åŒbatch sizeæµ‹è¯•
        print("\nğŸ“Š æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•:")
        batch_sizes = [1, 2, 4, 8, 16, 32]

        for batch_size in batch_sizes:
            try:
                # åˆ›å»ºbatchè¾“å…¥
                batch_input = np.repeat(single_input, batch_size, axis=0)

                # é¢„çƒ­
                for _ in range(5):
                    _ = self.sess.run([out.name for out in self.sess.get_outputs()],
                                      {self.sess.get_inputs()[0].name: batch_input})

                # æµ‹è¯•
                start_time = time.time()
                test_runs_batch = max(1, 20 // batch_size)  # é€‚åº”ä¸åŒbatch size
                for _ in range(test_runs_batch):
                    _ = self.sess.run([out.name for out in self.sess.get_outputs()],
                                      {self.sess.get_inputs()[0].name: batch_input})

                batch_time = (time.time() - start_time) / test_runs_batch
                per_frame_time = batch_time / batch_size
                batch_fps = batch_size / batch_time

                print(
                    f"Batch {batch_size:2d}: {batch_time * 1000:6.1f}ms total, {per_frame_time * 1000:5.1f}ms/frame, {batch_fps:5.1f} FPS")

            except Exception as e:
                print(f"Batch {batch_size}: å†…å­˜ä¸è¶³ - {e}")
                break

        # 5. å†…å­˜ä½¿ç”¨åˆ†æ
        print("\nğŸ“Š å†…å­˜ä½¿ç”¨åˆ†æ:")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            memory_before = torch.cuda.memory_allocated() / 1024 ** 2

            # æ‰§è¡Œä¸€æ¬¡å¤§batchæ¨ç†
            large_batch = np.repeat(single_input, 16, axis=0)
            _ = self.sess.run([out.name for out in self.sess.get_outputs()],
                              {self.sess.get_inputs()[0].name: large_batch})

            memory_after = torch.cuda.memory_allocated() / 1024 ** 2
            memory_peak = torch.cuda.max_memory_allocated() / 1024 ** 2

            print(f"æ¨ç†å‰GPUå†…å­˜: {memory_before:.1f}MB")
            print(f"æ¨ç†åGPUå†…å­˜: {memory_after:.1f}MB")
            print(f"å†…å­˜å³°å€¼: {memory_peak:.1f}MB")
            print(f"å†…å­˜å¢é‡: {memory_after - memory_before:.1f}MB")

        # 6. æ¨¡å‹ä¿¡æ¯åˆ†æ
        print("\nğŸ“Š æ¨¡å‹ç»“æ„åˆ†æ:")
        print(f"è¾“å…¥èŠ‚ç‚¹æ•°: {len(self.sess.get_inputs())}")
        print(f"è¾“å‡ºèŠ‚ç‚¹æ•°: {len(self.sess.get_outputs())}")

        for i, input_node in enumerate(self.sess.get_inputs()):
            print(f"è¾“å…¥ {i}: {input_node.name}, å½¢çŠ¶: {input_node.shape}, ç±»å‹: {input_node.type}")

        for i, output_node in enumerate(self.sess.get_outputs()):
            print(f"è¾“å‡º {i}: {output_node.name}, å½¢çŠ¶: {output_node.shape}, ç±»å‹: {output_node.type}")

        # 7. å¯¹æ¯”åˆ†æ
        print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”åˆ†æ:")
        expected_single_frame = 8  # RTMPose-M expected time (ms)
        expected_batch_16 = 5  # Expected per-frame time in batch

        single_frame_ms = single_frame_time * 1000
        efficiency_single = expected_single_frame / single_frame_ms

        print(
            f"å•å¸§æ€§èƒ½æ•ˆç‡: {efficiency_single:.2f}x (æœŸæœ›: {expected_single_frame}ms, å®é™…: {single_frame_ms:.1f}ms)")

        if single_frame_ms > 20:
            print("âš ï¸  å•å¸§æ€§èƒ½æ˜æ˜¾åæ…¢ï¼Œå¯èƒ½é—®é¢˜:")
            print("   - ONNXæ¨¡å‹æœªå……åˆ†ä¼˜åŒ–")
            print("   - ONNX Runtimeé…ç½®é—®é¢˜")
            print("   - è¾“å…¥æ•°æ®æ ¼å¼æˆ–ç²¾åº¦é—®é¢˜")

        print("\n" + "=" * 60)

    # åœ¨ ONNXPoseEstimator ç±»ä¸­æ·»åŠ æµ‹è¯•æ–¹æ³•
    def run_performance_test(self):
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        self.benchmark_detailed_performance()

    # å¿«é€Ÿæ·»åŠ åˆ°ç°æœ‰ä»£ç ä¸­è¿›è¡Œæµ‹è¯•
    def quick_performance_check(onnx_file_path, device='cuda'):
        """å¿«é€Ÿæ€§èƒ½æ£€æŸ¥å‡½æ•°"""
        from app.pose_service.onnx_inference import ONNXPoseEstimator

        pose_estimator = ONNXPoseEstimator(onnx_file_path, device)
        pose_estimator.benchmark_detailed_performance()

        return pose_estimator

# pose_estimator = ONNXPoseEstimator(
#     '/home/stanley/jobs/python/AI/fastapi_mmpose/app/pose_service/configs/rtmpose_onnx/end2end.onnx', 'cuda')
# pose_estimator.benchmark_detailed_performance()
